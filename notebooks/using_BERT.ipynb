{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\egzlz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Descargar el conjunto de datos 'punkt', que es necesario para la tokenización de oraciones\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.insert(0, 'C:\\\\Users\\\\egzlz\\\\AppData\\\\Roaming\\\\nltk_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\egzlz\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\Users\\\\egzlz/nltk_data', 'c:\\\\Users\\\\egzlz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\nltk_data', 'c:\\\\Users\\\\egzlz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\share\\\\nltk_data', 'c:\\\\Users\\\\egzlz\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\egzlz\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\egzlz\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPiece Tokenizer (BERT):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egzlz\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unbelievable']\n",
      "\n",
      "Byte-Pair Encoding Tokenizer (RoBERTa):\n",
      "['un', 'bel', 'iev', 'able']\n",
      "\n",
      "SentencePiece Tokenizer (ALBERT):\n",
      "['▁un', 'be', 'liev', 'able']\n",
      "\n",
      "Unigram Tokenizer (T5):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁unbelievable']\n",
      "\n",
      "Fast Tokenizer (BERT Fast):\n",
      "['unbelievable']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, RobertaTokenizer, AlbertTokenizer, T5Tokenizer, BertTokenizerFast\n",
    "\n",
    "# Texto de ejemplo\n",
    "text = \"unbelievable\"\n",
    "\n",
    "# 1. WordPiece Tokenizer (BERT)\n",
    "print(\"WordPiece Tokenizer (BERT):\")\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens_bert = tokenizer_bert.tokenize(text)\n",
    "print(tokens_bert)\n",
    "\n",
    "# 2. Byte-Pair Encoding (BPE) Tokenizer (RoBERTa)\n",
    "print(\"\\nByte-Pair Encoding Tokenizer (RoBERTa):\")\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tokens_roberta = tokenizer_roberta.tokenize(text)\n",
    "print(tokens_roberta)\n",
    "\n",
    "# 3. SentencePiece Tokenizer (ALBERT)\n",
    "print(\"\\nSentencePiece Tokenizer (ALBERT):\")\n",
    "tokenizer_albert = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "tokens_albert = tokenizer_albert.tokenize(text)\n",
    "print(tokens_albert)\n",
    "\n",
    "# 4. Unigram Tokenizer (T5)\n",
    "print(\"\\nUnigram Tokenizer (T5):\")\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained('t5-small')\n",
    "tokens_t5 = tokenizer_t5.tokenize(text)\n",
    "print(tokens_t5)\n",
    "\n",
    "# 5. Fast Tokenizer (BERT Fast)\n",
    "print(\"\\nFast Tokenizer (BERT Fast):\")\n",
    "tokenizer_fast = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "tokens_fast = tokenizer_fast.tokenize(text)\n",
    "print(tokens_fast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'How did serfdom develop in and then leave Russia ?', 'coarse_label': 2, 'fine_label': 26}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, get_dataset_infos\n",
    "\n",
    "\n",
    "# Para obtener más detalles de un dataset específico\n",
    "dataset = load_dataset('trec',trust_remote_code=True)\n",
    "print(dataset['train'][0])\n",
    "train_dataset=dataset['train']\n",
    "test_dataset=dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABBR:abb', 'ABBR:exp', 'ENTY:animal', 'ENTY:body', 'ENTY:color', 'ENTY:cremat', 'ENTY:currency', 'ENTY:dismed', 'ENTY:event', 'ENTY:food', 'ENTY:instru', 'ENTY:lang', 'ENTY:letter', 'ENTY:other', 'ENTY:plant', 'ENTY:product', 'ENTY:religion', 'ENTY:sport', 'ENTY:substance', 'ENTY:symbol', 'ENTY:techmeth', 'ENTY:termeq', 'ENTY:veh', 'ENTY:word', 'DESC:def', 'DESC:desc', 'DESC:manner', 'DESC:reason', 'HUM:gr', 'HUM:ind', 'HUM:title', 'HUM:desc', 'LOC:city', 'LOC:country', 'LOC:mount', 'LOC:other', 'LOC:state', 'NUM:code', 'NUM:count', 'NUM:date', 'NUM:dist', 'NUM:money', 'NUM:ord', 'NUM:other', 'NUM:period', 'NUM:perc', 'NUM:speed', 'NUM:temp', 'NUM:volsize', 'NUM:weight']\n",
      "['ABBR', 'ENTY', 'DESC', 'HUM', 'LOC', 'NUM']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'].features['fine_label'].names)\n",
    "print(dataset['train'].features['coarse_label'].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5452/5452 [00:00<00:00, 15895.08 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 10418.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,BertTokenizerFast\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to tokenize the text\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], padding='max_length', truncation=True, max_length=64)\n",
    "\n",
    "# Apply the tokenizer to the dataset\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove the original text columns (optional)\n",
    "train_dataset = train_dataset.remove_columns(['text'])\n",
    "test_dataset = test_dataset.remove_columns(['text'])\n",
    "\n",
    "# Convert labels to TensorFlow format\n",
    "train_dataset = train_dataset.with_format(\"tensorflow\")\n",
    "test_dataset = test_dataset.with_format(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de CUDA: 64_112\n",
      "Versión de cuDNN: 64_8\n"
     ]
    }
   ],
   "source": [
    "print(\"Versión de CUDA:\", tf.sysconfig.get_build_info()[\"cuda_version\"])\n",
    "print(\"Versión de cuDNN:\", tf.sysconfig.get_build_info()[\"cudnn_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9120492276557268230\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12871954455675869555\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2915486926\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7408883155114554843\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Configurar TensorFlow para usar la primera GPU disponible\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        print(\"Using GPU: \", gpus[0])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert to TensorFlow datasets\n",
    "train_features = {x: tf.convert_to_tensor(train_dataset[x]) for x in tokenizer.model_input_names}\n",
    "train_labels = tf.convert_to_tensor(train_dataset['fine_label'])\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels)).shuffle(1000).batch(16)\n",
    "\n",
    "test_features = {x: tf.convert_to_tensor(test_dataset[x]) for x in tokenizer.model_input_names}\n",
    "test_labels = tf.convert_to_tensor(test_dataset['fine_label'])\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels)).batch(16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification,TFDistilBertForSequenceClassification\n",
    "\n",
    "# Load the BERT model for sequence classification\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=50)\n",
    "\n",
    "#model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mds_train\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ds_train' is not defined"
     ]
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5452,), dtype=int64, numpy=array([26,  5, 26, ..., 47, 47,  6], dtype=int64)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5452, 64)\n",
      "(5452, 64)\n",
      "(5452,)\n"
     ]
    }
   ],
   "source": [
    "print(train_features['input_ids'].shape)  # Asegúrate de que estas formas sean correctas\n",
    "print(train_features['attention_mask'].shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: {'input_ids': <tf.Tensor: shape=(16, 64), dtype=int64, numpy=\n",
      "array([[ 101, 2054, 3146, ...,    0,    0,    0],\n",
      "       [ 101, 2054, 2103, ...,    0,    0,    0],\n",
      "       [ 101, 2054, 2003, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 101, 2054, 2024, ...,    0,    0,    0],\n",
      "       [ 101, 2054, 2003, ...,    0,    0,    0],\n",
      "       [ 101, 2129, 2116, ...,    0,    0,    0]], dtype=int64)>, 'attention_mask': <tf.Tensor: shape=(16, 64), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int64)>}\n",
      "labels: [32 32 47 26 39 26 25 13 11  7 13 39  5 20 24 38]\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in train_dataset.take(1):\n",
    "    print(f\"inputs: {inputs}\")\n",
    "    print(f\"labels: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=({'input_ids': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.convert_to_tensor(train_labels)\n",
    "test_labels = tf.convert_to_tensor(test_labels)\n",
    "\n",
    "# Eliminar 'token_type_ids' de los inputs, ya que DistilBERT no lo necesita\n",
    "train_features = {key: value for key, value in train_features.items() if key != 'token_type_ids'}\n",
    "test_features = {key: value for key, value in test_features.items() if key != 'token_type_ids'}\n",
    "\n",
    "# Asegurarse de que los datasets estén en el formato correcto\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels)).shuffle(1000).batch(16)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels)).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "341/341 [==============================] - 118s 311ms/step - loss: 2.2806 - accuracy: 0.5163 - val_loss: 1.4076 - val_accuracy: 0.6860\n",
      "Epoch 2/3\n",
      "341/341 [==============================] - 103s 303ms/step - loss: 0.9121 - accuracy: 0.8081 - val_loss: 0.8941 - val_accuracy: 0.7980\n",
      "Epoch 3/3\n",
      "341/341 [==============================] - 103s 303ms/step - loss: 0.4821 - accuracy: 0.9006 - val_loss: 0.6555 - val_accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=3,\n",
    "    batch_size=16,  # Ajustar según la memoria de tu GPU\n",
    "    validation_data=test_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 3s 165ms/step - loss: 0.6555 - accuracy: 0.8500\n",
      "Loss: 0.65546053647995, Accuracy: 0.8500000238418579\n",
      "16/16 [==============================] - 3s 161ms/step\n",
      "Predicted labels: [40 35 31 24 39 40 28  2 27 24]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Evaluar el modelo\n",
    "loss, accuracy = model.evaluate(test_features, test_labels)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
    "\n",
    "# Predecir usando el modelo\n",
    "predictions = model.predict(test_features)\n",
    "predicted_labels = np.argmax(predictions.logits, axis=-1)\n",
    "\n",
    "# Convertir predicciones numéricas de vuelta a etiquetas\n",
    "#label_names = ds_info.features['label-coarse'].int2str\n",
    "#predicted_labels = [label_names(label) for label in predicted_labels]\n",
    "print(\"Predicted labels:\", predicted_labels[:10])  # Muestra las primeras 10 predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_label_mapping = dataset['train'].features['fine_label'].names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=({'input_ids': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones Numéricas: [40 35 31 24 39 40 28  2 27 24 32 29 43 29 39 44 18 29 24 39 29 24 35 24\n",
      " 24 29 24 35 35 35 40 35 24 43 39 44 35 40 38 38  9 24 24 24 29 21 44 24\n",
      " 38 28 24 29 29 24 25 24 39 32 35 43 32 29 35  9 36 31  2 24 39 35 43 41\n",
      " 32 29 35 24 41 40 41 29 24 39 29 43 39 27 24 29 24 33 13 24 43 39 28 35\n",
      " 39 24 24  8 24 29 26 31 35 24 43  2 39 35  4 24 40 29  2 29 39 18 39 35\n",
      " 39 40 24 29 40 29 40 39 24 44 40 29 24 22 24 35 35 29 13 24 24  2 35 21\n",
      " 24 13 29 22 24 24 21 29 24 39 28 35 32 24 35 36 24 24 39 35 21 35 39 24\n",
      " 36 29 39 40 39 24 35 36 35 39  4 40 35 24 35 24 18 29 29 24 35 13 41 40\n",
      " 44 39 24 32 41 29 13 18 22 32 29  2 43 35 29 43 25 25 39 40 36 35 24 11\n",
      " 24 21 38 24 24 39 24 43 34 18 43 25 35 29 39 29 24 38 28 24 35 24 35 40\n",
      " 36 24 27 24 24 35 24 24 25 25 29 24 13 18 24 35 40 21 24 41 29 29 38 13\n",
      " 44 24 24 32 44 26 11 25 24 29 24 27 24 39  1 18 35 29  1 24 35 24 24 24\n",
      " 24 24 32 24 44 21 24 24  4 44 13  4  1 39 24 39 39 35 29 35 29 39 32 35\n",
      " 38 24 35 24 29 35  2 35 24 39 24  4 39 24 27 43 39 24 24 39  2 22 43 35\n",
      " 24  9 24 43 41 28 43 29 43 29 13 25 39 24 39 35  9  2 24 29 43 40 27  2\n",
      " 24 24 35 38 24 40  4  4 24 32 43 41 24 24  9 39 35 24  1 24 29 29 13 29\n",
      " 44 39 24 29 24 24 29 24  2 38 24 24 24 41 33 34 29 18 39  1 40 32 24 35\n",
      " 39 24 29 18 32  1  2 24 24 24 29 29 39 35 22 24 35 35 24 35 24 29 24  2\n",
      " 35 29 24 24 24 39 24  2 29 13 33  2 29  2 35 29 39  2  4 17  9 39 26 39\n",
      "  4 35 24 24 13  2 35 39 36 21 24 28 24 24 24 24 41 43 18 41 24 32  4 33\n",
      " 22 18 18 24 35 24 32 40 13 22 24 24 20 39 38 29 41 38 13 24]\n",
      "Nombres de Etiquetas Mapeadas: ['NUM:dist', 'LOC:other', 'HUM:desc', 'DESC:def', 'NUM:date', 'NUM:dist', 'HUM:gr', 'ENTY:animal', 'DESC:reason', 'DESC:def', 'LOC:city', 'HUM:ind', 'NUM:other', 'HUM:ind', 'NUM:date', 'NUM:period', 'ENTY:substance', 'HUM:ind', 'DESC:def', 'NUM:date', 'HUM:ind', 'DESC:def', 'LOC:other', 'DESC:def', 'DESC:def', 'HUM:ind', 'DESC:def', 'LOC:other', 'LOC:other', 'LOC:other', 'NUM:dist', 'LOC:other', 'DESC:def', 'NUM:other', 'NUM:date', 'NUM:period', 'LOC:other', 'NUM:dist', 'NUM:count', 'NUM:count', 'ENTY:food', 'DESC:def', 'DESC:def', 'DESC:def', 'HUM:ind', 'ENTY:termeq', 'NUM:period', 'DESC:def', 'NUM:count', 'HUM:gr', 'DESC:def', 'HUM:ind', 'HUM:ind', 'DESC:def', 'DESC:desc', 'DESC:def', 'NUM:date', 'LOC:city', 'LOC:other', 'NUM:other', 'LOC:city', 'HUM:ind', 'LOC:other', 'ENTY:food', 'LOC:state', 'HUM:desc', 'ENTY:animal', 'DESC:def', 'NUM:date', 'LOC:other', 'NUM:other', 'NUM:money', 'LOC:city', 'HUM:ind', 'LOC:other', 'DESC:def', 'NUM:money', 'NUM:dist', 'NUM:money', 'HUM:ind', 'DESC:def', 'NUM:date', 'HUM:ind', 'NUM:other', 'NUM:date', 'DESC:reason', 'DESC:def', 'HUM:ind', 'DESC:def', 'LOC:country', 'ENTY:other', 'DESC:def', 'NUM:other', 'NUM:date', 'HUM:gr', 'LOC:other', 'NUM:date', 'DESC:def', 'DESC:def', 'ENTY:event', 'DESC:def', 'HUM:ind', 'DESC:manner', 'HUM:desc', 'LOC:other', 'DESC:def', 'NUM:other', 'ENTY:animal', 'NUM:date', 'LOC:other', 'ENTY:color', 'DESC:def', 'NUM:dist', 'HUM:ind', 'ENTY:animal', 'HUM:ind', 'NUM:date', 'ENTY:substance', 'NUM:date', 'LOC:other', 'NUM:date', 'NUM:dist', 'DESC:def', 'HUM:ind', 'NUM:dist', 'HUM:ind', 'NUM:dist', 'NUM:date', 'DESC:def', 'NUM:period', 'NUM:dist', 'HUM:ind', 'DESC:def', 'ENTY:veh', 'DESC:def', 'LOC:other', 'LOC:other', 'HUM:ind', 'ENTY:other', 'DESC:def', 'DESC:def', 'ENTY:animal', 'LOC:other', 'ENTY:termeq', 'DESC:def', 'ENTY:other', 'HUM:ind', 'ENTY:veh', 'DESC:def', 'DESC:def', 'ENTY:termeq', 'HUM:ind', 'DESC:def', 'NUM:date', 'HUM:gr', 'LOC:other', 'LOC:city', 'DESC:def', 'LOC:other', 'LOC:state', 'DESC:def', 'DESC:def', 'NUM:date', 'LOC:other', 'ENTY:termeq', 'LOC:other', 'NUM:date', 'DESC:def', 'LOC:state', 'HUM:ind', 'NUM:date', 'NUM:dist', 'NUM:date', 'DESC:def', 'LOC:other', 'LOC:state', 'LOC:other', 'NUM:date', 'ENTY:color', 'NUM:dist', 'LOC:other', 'DESC:def', 'LOC:other', 'DESC:def', 'ENTY:substance', 'HUM:ind', 'HUM:ind', 'DESC:def', 'LOC:other', 'ENTY:other', 'NUM:money', 'NUM:dist', 'NUM:period', 'NUM:date', 'DESC:def', 'LOC:city', 'NUM:money', 'HUM:ind', 'ENTY:other', 'ENTY:substance', 'ENTY:veh', 'LOC:city', 'HUM:ind', 'ENTY:animal', 'NUM:other', 'LOC:other', 'HUM:ind', 'NUM:other', 'DESC:desc', 'DESC:desc', 'NUM:date', 'NUM:dist', 'LOC:state', 'LOC:other', 'DESC:def', 'ENTY:lang', 'DESC:def', 'ENTY:termeq', 'NUM:count', 'DESC:def', 'DESC:def', 'NUM:date', 'DESC:def', 'NUM:other', 'LOC:mount', 'ENTY:substance', 'NUM:other', 'DESC:desc', 'LOC:other', 'HUM:ind', 'NUM:date', 'HUM:ind', 'DESC:def', 'NUM:count', 'HUM:gr', 'DESC:def', 'LOC:other', 'DESC:def', 'LOC:other', 'NUM:dist', 'LOC:state', 'DESC:def', 'DESC:reason', 'DESC:def', 'DESC:def', 'LOC:other', 'DESC:def', 'DESC:def', 'DESC:desc', 'DESC:desc', 'HUM:ind', 'DESC:def', 'ENTY:other', 'ENTY:substance', 'DESC:def', 'LOC:other', 'NUM:dist', 'ENTY:termeq', 'DESC:def', 'NUM:money', 'HUM:ind', 'HUM:ind', 'NUM:count', 'ENTY:other', 'NUM:period', 'DESC:def', 'DESC:def', 'LOC:city', 'NUM:period', 'DESC:manner', 'ENTY:lang', 'DESC:desc', 'DESC:def', 'HUM:ind', 'DESC:def', 'DESC:reason', 'DESC:def', 'NUM:date', 'ABBR:exp', 'ENTY:substance', 'LOC:other', 'HUM:ind', 'ABBR:exp', 'DESC:def', 'LOC:other', 'DESC:def', 'DESC:def', 'DESC:def', 'DESC:def', 'DESC:def', 'LOC:city', 'DESC:def', 'NUM:period', 'ENTY:termeq', 'DESC:def', 'DESC:def', 'ENTY:color', 'NUM:period', 'ENTY:other', 'ENTY:color', 'ABBR:exp', 'NUM:date', 'DESC:def', 'NUM:date', 'NUM:date', 'LOC:other', 'HUM:ind', 'LOC:other', 'HUM:ind', 'NUM:date', 'LOC:city', 'LOC:other', 'NUM:count', 'DESC:def', 'LOC:other', 'DESC:def', 'HUM:ind', 'LOC:other', 'ENTY:animal', 'LOC:other', 'DESC:def', 'NUM:date', 'DESC:def', 'ENTY:color', 'NUM:date', 'DESC:def', 'DESC:reason', 'NUM:other', 'NUM:date', 'DESC:def', 'DESC:def', 'NUM:date', 'ENTY:animal', 'ENTY:veh', 'NUM:other', 'LOC:other', 'DESC:def', 'ENTY:food', 'DESC:def', 'NUM:other', 'NUM:money', 'HUM:gr', 'NUM:other', 'HUM:ind', 'NUM:other', 'HUM:ind', 'ENTY:other', 'DESC:desc', 'NUM:date', 'DESC:def', 'NUM:date', 'LOC:other', 'ENTY:food', 'ENTY:animal', 'DESC:def', 'HUM:ind', 'NUM:other', 'NUM:dist', 'DESC:reason', 'ENTY:animal', 'DESC:def', 'DESC:def', 'LOC:other', 'NUM:count', 'DESC:def', 'NUM:dist', 'ENTY:color', 'ENTY:color', 'DESC:def', 'LOC:city', 'NUM:other', 'NUM:money', 'DESC:def', 'DESC:def', 'ENTY:food', 'NUM:date', 'LOC:other', 'DESC:def', 'ABBR:exp', 'DESC:def', 'HUM:ind', 'HUM:ind', 'ENTY:other', 'HUM:ind', 'NUM:period', 'NUM:date', 'DESC:def', 'HUM:ind', 'DESC:def', 'DESC:def', 'HUM:ind', 'DESC:def', 'ENTY:animal', 'NUM:count', 'DESC:def', 'DESC:def', 'DESC:def', 'NUM:money', 'LOC:country', 'LOC:mount', 'HUM:ind', 'ENTY:substance', 'NUM:date', 'ABBR:exp', 'NUM:dist', 'LOC:city', 'DESC:def', 'LOC:other', 'NUM:date', 'DESC:def', 'HUM:ind', 'ENTY:substance', 'LOC:city', 'ABBR:exp', 'ENTY:animal', 'DESC:def', 'DESC:def', 'DESC:def', 'HUM:ind', 'HUM:ind', 'NUM:date', 'LOC:other', 'ENTY:veh', 'DESC:def', 'LOC:other', 'LOC:other', 'DESC:def', 'LOC:other', 'DESC:def', 'HUM:ind', 'DESC:def', 'ENTY:animal', 'LOC:other', 'HUM:ind', 'DESC:def', 'DESC:def', 'DESC:def', 'NUM:date', 'DESC:def', 'ENTY:animal', 'HUM:ind', 'ENTY:other', 'LOC:country', 'ENTY:animal', 'HUM:ind', 'ENTY:animal', 'LOC:other', 'HUM:ind', 'NUM:date', 'ENTY:animal', 'ENTY:color', 'ENTY:sport', 'ENTY:food', 'NUM:date', 'DESC:manner', 'NUM:date', 'ENTY:color', 'LOC:other', 'DESC:def', 'DESC:def', 'ENTY:other', 'ENTY:animal', 'LOC:other', 'NUM:date', 'LOC:state', 'ENTY:termeq', 'DESC:def', 'HUM:gr', 'DESC:def', 'DESC:def', 'DESC:def', 'DESC:def', 'NUM:money', 'NUM:other', 'ENTY:substance', 'NUM:money', 'DESC:def', 'LOC:city', 'ENTY:color', 'LOC:country', 'ENTY:veh', 'ENTY:substance', 'ENTY:substance', 'DESC:def', 'LOC:other', 'DESC:def', 'LOC:city', 'NUM:dist', 'ENTY:other', 'ENTY:veh', 'DESC:def', 'DESC:def', 'ENTY:techmeth', 'NUM:date', 'NUM:count', 'HUM:ind', 'NUM:money', 'NUM:count', 'ENTY:other', 'DESC:def']\n"
     ]
    }
   ],
   "source": [
    "predicted_label_names = [fine_label_mapping[label] for label in predicted_labels]\n",
    "test_features\n",
    "# Mostrar los resultados mapeados\n",
    "for i in range(0,10):\n",
    "print(\"Predicciones Numéricas:\", predicted_labels[i])\n",
    "\n",
    "print(\"Nombres de Etiquetas Mapeadas:\", predicted_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice: 0\n",
      "Texto original: How far is it from Denver to Aspen ?\n",
      "Etiqueta Original Numérica: 40\n",
      "Prediccion  Numérica: 40\n",
      "Etiqueta Original Nombre: NUM:dist\n",
      "Prediccion  Nombre: NUM:dist\n",
      "==================================================\n",
      "Índice: 1\n",
      "Texto original: What county is Modesto , California in ?\n",
      "Etiqueta Original Numérica: 35\n",
      "Prediccion  Numérica: 32\n",
      "Etiqueta Original Nombre: LOC:city\n",
      "Prediccion  Nombre: LOC:other\n",
      "==================================================\n",
      "Índice: 2\n",
      "Texto original: Who was Galileo ?\n",
      "Etiqueta Original Numérica: 31\n",
      "Prediccion  Numérica: 31\n",
      "Etiqueta Original Nombre: HUM:desc\n",
      "Prediccion  Nombre: HUM:desc\n",
      "==================================================\n",
      "Índice: 3\n",
      "Texto original: What is an atom ?\n",
      "Etiqueta Original Numérica: 24\n",
      "Prediccion  Numérica: 24\n",
      "Etiqueta Original Nombre: DESC:def\n",
      "Prediccion  Nombre: DESC:def\n",
      "==================================================\n",
      "Índice: 4\n",
      "Texto original: When did Hawaii become a state ?\n",
      "Etiqueta Original Numérica: 39\n",
      "Prediccion  Numérica: 39\n",
      "Etiqueta Original Nombre: NUM:date\n",
      "Prediccion  Nombre: NUM:date\n",
      "==================================================\n",
      "Índice: 5\n",
      "Texto original: How tall is the Sears Building ?\n",
      "Etiqueta Original Numérica: 40\n",
      "Prediccion  Numérica: 40\n",
      "Etiqueta Original Nombre: NUM:dist\n",
      "Prediccion  Nombre: NUM:dist\n",
      "==================================================\n",
      "Índice: 6\n",
      "Texto original: George Bush purchased a small interest in which baseball team ?\n",
      "Etiqueta Original Numérica: 28\n",
      "Prediccion  Numérica: 28\n",
      "Etiqueta Original Nombre: HUM:gr\n",
      "Prediccion  Nombre: HUM:gr\n",
      "==================================================\n",
      "Índice: 7\n",
      "Texto original: What is Australia 's national flower ?\n",
      "Etiqueta Original Numérica: 2\n",
      "Prediccion  Numérica: 14\n",
      "Etiqueta Original Nombre: ENTY:plant\n",
      "Prediccion  Nombre: ENTY:animal\n",
      "==================================================\n",
      "Índice: 8\n",
      "Texto original: Why does the moon turn orange ?\n",
      "Etiqueta Original Numérica: 27\n",
      "Prediccion  Numérica: 27\n",
      "Etiqueta Original Nombre: DESC:reason\n",
      "Prediccion  Nombre: DESC:reason\n",
      "==================================================\n",
      "Índice: 9\n",
      "Texto original: What is autism ?\n",
      "Etiqueta Original Numérica: 24\n",
      "Prediccion  Numérica: 24\n",
      "Etiqueta Original Nombre: DESC:def\n",
      "Prediccion  Nombre: DESC:def\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Mostrar los textos originales y las predicciones\n",
    "for idx in range(0,10):\n",
    "    original_text = dataset['test'][idx]['text']\n",
    "    original_label = dataset['test'][idx]['fine_label']\n",
    "    label_name = dataset['train'].features['fine_label'].int2str(original_label)\n",
    "    precition_number=predicted_labels[idx]\n",
    "    precition_name=predicted_label_names[idx]\n",
    "    print(f\"Índice: {idx}\")\n",
    "    print(f\"Texto original: {original_text}\")\n",
    "    print(f\"Etiqueta Original Numérica: {precition_number}\")\n",
    "    print(f\"Prediccion  Numérica: {original_label}\")\n",
    "    print(f\"Etiqueta Original Nombre: {label_name}\")\n",
    "    print(f\"Prediccion  Nombre: {precition_name}\")\n",
    "    \n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
